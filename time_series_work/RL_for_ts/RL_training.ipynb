{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import random\n",
    "from operator import itemgetter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate, state_shape, action_shape, n_dense_1, n_dense_2):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.layer_1 =  nn.Linear(state_shape, n_dense_1)\n",
    "        self.layer_2 =  nn.Linear(n_dense_1, n_dense_2)\n",
    "        self.layer_3 =  nn.Linear(n_dense_2, action_shape, dtype= torch.float32)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr = learning_rate)\n",
    "        self.cost_func = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.layer_1(state))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        actions = self.layer_3(x)\n",
    "        return actions\n",
    "\n",
    "@dataclass\n",
    "class RLModelWrapper:\n",
    "    state_shape:int\n",
    "    is_training:bool\n",
    "\n",
    "    current_state_memory:list = []\n",
    "    nxt_state_memory:list = []\n",
    "    actions_taken_memory: list = []\n",
    "    action_indexes_memory : list = []\n",
    "    reward_memory:list = []\n",
    "    iteration_done_bool_memory: list = []\n",
    "\n",
    "    EPSILON:float = 0.5 if is_training else 0\n",
    "    epsilon_min:float = 0.01\n",
    "    epsilon_decay:float = 0.01\n",
    "    ALPHA:float= 0.05\n",
    "    learning_rate:float = 0.001\n",
    "    DISCOUNT:float = 0.95\n",
    "    cost_tracker:float = 0\n",
    "\n",
    "    start_action: float = 0.05\n",
    "    end_action: float = 2.0\n",
    "    action_gap: float = 0.05\n",
    "\n",
    "    agent_actions: list = np.arange(start=start_action,stop=end_action,step=action_gap)\n",
    "    n_actions: int = len(agent_actions)\n",
    "\n",
    "\n",
    "    batch_size:int = 80\n",
    "    batch_start:int = 0\n",
    "    batch_end:int = batch_size\n",
    "\n",
    "    main_model: DeepQNetwork = DeepQNetwork(learning_rate,state_shape, n_actions, n_dense_1=100, n_dense_2=100)\n",
    "    target_model: DeepQNetwork = DeepQNetwork(learning_rate, state_shape, n_actions, n_dense_1=100, n_dense_2=100)\n",
    "    target_model.load_state_dict(main_model.state_dict())\n",
    "\n",
    "    def store_data(self, curr_obs, nxt_obs, action, reward, done) -> None:\n",
    "        self.current_state_memory.append(curr_obs)\n",
    "        self.nxt_state_memory.append(nxt_obs)\n",
    "        self.actions_taken_memory.append(action)\n",
    "        self.action_indexes_memory.append(self.action_mapping(action, reversed= True))\n",
    "        self.reward_memory.append(reward)\n",
    "\n",
    "        done_val = 1 if done else 0\n",
    "        self.iteration_done_bool_memory.append(done_val)\n",
    "\n",
    "    def action_mapping(self,action_or_index , reversed = False) -> int | float: \n",
    "        if reversed:\n",
    "            # if want to get action index by the specific action\n",
    "            return self.agent_actions.index(action_or_index)\n",
    "        else:\n",
    "            #if want to get action by specific action index\n",
    "            return self.agent_actions[action_or_index]\n",
    "\n",
    "    def take_action(self,observation):\n",
    "        if np.random.uniform() > self.EPSILON:\n",
    "            encoded = observation\n",
    "            predicted = self.main_model.forward(torch.tensor(encoded))\n",
    "            action_index = np.argmax(predicted.tolist())\n",
    "            return self.action_mapping(action_index)\n",
    "        else:\n",
    "            action_index = random.choice(range(self.n_actions))\n",
    "            return self.action_mapping(action_index)\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        current_cost_tracker = 0\n",
    "        MIN_REPLAY_SIZE = self.batch_end + self.batch_size\n",
    "        if len(self.iteration_done_bool_memory) < MIN_REPLAY_SIZE:\n",
    "            return\n",
    "        else:\n",
    "            batch_sample = random.sample(list(range(len(self.dones_mem))), self.batch_size)\n",
    "            batch_indexes = list(range(self.batch_size))\n",
    "\n",
    "            self.main_model.optimizer.zero_grad()\n",
    "            curr_obs_batch = torch.tensor(itemgetter(*batch_sample)(self.current_state_memory), dtype= torch.float32)\n",
    "            nxt_obs_batch  = torch.tensor(itemgetter(*batch_sample)(self.nxt_state_memory), dtype= torch.float32)\n",
    "            actions_indexes_batch = itemgetter(*batch_sample)(self.action_indexes_memory)\n",
    "            rewards_batch = torch.tensor(itemgetter(*batch_sample)(self.reward_memory), dtype= torch.float32)\n",
    "            dones_batch = itemgetter(*batch_sample)(self.iteration_done_bool_memory)  # [0,0,0,1]\n",
    "            not_dones_batch = torch.tensor(np.ones(len(dones_batch))-dones_batch, dtype= torch.float32) # [1,1,1,0]\n",
    "            \n",
    "            curr_qs_batch = self.main_model.forward(curr_obs_batch)[batch_indexes, actions_indexes_batch]\n",
    "            nxt_qs_batch = self.target_model.forward(nxt_obs_batch)\n",
    "\n",
    "            q_target = torch.add(rewards_batch,not_dones_batch*self.DISCOUNT*torch.max(nxt_qs_batch, dim=1)[0])\n",
    "            # changed_q = curr_qs_batch + ALPHA * (q_target - curr_qs_batch)\n",
    "            cost = self.main_model.cost_func(curr_qs_batch, q_target)\n",
    "            current_cost_tracker+=cost\n",
    "            cost.backward()\n",
    "            self.main_model.optimizer.step()\n",
    "\n",
    "            self.batch_start = self.batch_end\n",
    "            self.batch_end += self.batch_size\n",
    "            self.cost_tracker = current_cost_tracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.25 , ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(1,2,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 12, 15)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_lst = list(range(10,21))\n",
    "\n",
    "itemgetter(*[0,2,5])(check_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

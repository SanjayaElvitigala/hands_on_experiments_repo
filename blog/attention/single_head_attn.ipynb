{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import copy, deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from sktime.performance_metrics.forecasting import (mean_absolute_percentage_error,\n",
    "                                                     mean_absolute_error)\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15f524d2b50>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the synthetic data\n",
    "row_count = 1000\n",
    "n_var = 20\n",
    "\n",
    "if n_var<=10:\n",
    "    step_val = 0.1/n_var\n",
    "else:\n",
    "    step_val = 0.05/n_var\n",
    "\n",
    "start_var = np.round(np.random.rand(row_count)*10,0).tolist()\n",
    "data_dct = {}\n",
    "\n",
    "for x_var_num in range(n_var):\n",
    "    data_dct[f'x{x_var_num}'] = []\n",
    "data_dct[f'x{n_var-1}'] = start_var\n",
    "\n",
    "all_cols = list(data_dct.keys())\n",
    "\n",
    "multiply_factors = np.arange(start=1, stop=2, step=step_val)\n",
    "\n",
    "# an implementation to produce cumulatively increasing array of values row wise\n",
    "for row_num in range(row_count):\n",
    "    factor_cpy = copy(multiply_factors)\n",
    "    for index, x_var_num in enumerate(range(n_var-2,-1,-1)):\n",
    "        # getting a window of factors as per the x_var_num\n",
    "        windowed_factors = factor_cpy[index:index+3]\n",
    "        last_used_factor = np.random.choice(a=windowed_factors,size=1)[0]\n",
    "\n",
    "        calc_value = np.round(data_dct[f'x{x_var_num+1}'][row_num]*last_used_factor,2)\n",
    "        \n",
    "        data_dct[f'x{x_var_num}'].append(calc_value)\n",
    "\n",
    "        factor_cpy = factor_cpy[factor_cpy>last_used_factor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134.17</td>\n",
       "      <td>87.98</td>\n",
       "      <td>59.65</td>\n",
       "      <td>41.71</td>\n",
       "      <td>30.06</td>\n",
       "      <td>22.39</td>\n",
       "      <td>17.22</td>\n",
       "      <td>13.67</td>\n",
       "      <td>11.16</td>\n",
       "      <td>9.38</td>\n",
       "      <td>8.10</td>\n",
       "      <td>7.18</td>\n",
       "      <td>6.50</td>\n",
       "      <td>5.99</td>\n",
       "      <td>5.61</td>\n",
       "      <td>5.34</td>\n",
       "      <td>5.17</td>\n",
       "      <td>5.06</td>\n",
       "      <td>5.01</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163.88</td>\n",
       "      <td>108.53</td>\n",
       "      <td>74.21</td>\n",
       "      <td>52.54</td>\n",
       "      <td>38.42</td>\n",
       "      <td>28.94</td>\n",
       "      <td>22.52</td>\n",
       "      <td>18.02</td>\n",
       "      <td>14.80</td>\n",
       "      <td>12.49</td>\n",
       "      <td>10.81</td>\n",
       "      <td>9.59</td>\n",
       "      <td>8.72</td>\n",
       "      <td>8.11</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.36</td>\n",
       "      <td>7.16</td>\n",
       "      <td>7.05</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131.15</td>\n",
       "      <td>87.58</td>\n",
       "      <td>60.40</td>\n",
       "      <td>42.99</td>\n",
       "      <td>31.55</td>\n",
       "      <td>23.90</td>\n",
       "      <td>18.71</td>\n",
       "      <td>15.06</td>\n",
       "      <td>12.45</td>\n",
       "      <td>10.55</td>\n",
       "      <td>9.17</td>\n",
       "      <td>8.19</td>\n",
       "      <td>7.46</td>\n",
       "      <td>6.94</td>\n",
       "      <td>6.56</td>\n",
       "      <td>6.31</td>\n",
       "      <td>6.16</td>\n",
       "      <td>6.07</td>\n",
       "      <td>6.02</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x0      x1     x2     x3     x4     x5     x6     x7     x8     x9  \\\n",
       "0  134.17   87.98  59.65  41.71  30.06  22.39  17.22  13.67  11.16   9.38   \n",
       "1  163.88  108.53  74.21  52.54  38.42  28.94  22.52  18.02  14.80  12.49   \n",
       "2  131.15   87.58  60.40  42.99  31.55  23.90  18.71  15.06  12.45  10.55   \n",
       "\n",
       "     x10   x11   x12   x13   x14   x15   x16   x17   x18  x19  \n",
       "0   8.10  7.18  6.50  5.99  5.61  5.34  5.17  5.06  5.01  5.0  \n",
       "1  10.81  9.59  8.72  8.11  7.67  7.36  7.16  7.05  7.00  7.0  \n",
       "2   9.17  8.19  7.46  6.94  6.56  6.31  6.16  6.07  6.02  6.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = pd.DataFrame(data_dct)\n",
    "main_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_diff_data = False\n",
    "\n",
    "if is_diff_data:\n",
    "    main_df = pd.DataFrame(np.round(main_df[all_cols[:-1]].to_numpy()-main_df[all_cols[1:]].to_numpy(),3), columns=all_cols[:-1])\n",
    "    n_var = len(main_df.columns)\n",
    "    all_cols = list(main_df.columns)\n",
    "    main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_indexes(batch_size, data_len, train_len):\n",
    "    total_batches = train_len//batch_size\n",
    "\n",
    "    batch_indexes = np.column_stack((np.arange(start=0,stop=data_len, step=batch_size),\n",
    "               np.arange(start=0+batch_size,stop=data_len+batch_size, step=batch_size)))\n",
    "    return batch_indexes[:total_batches]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_emb, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_emb).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_emb, 2).float() * -(math.log(10000.0) / d_emb)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)[:pe[:, 0::2].shape[0],:pe[:, 0::2].shape[1]]\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)[:pe[:, 1::2].shape[0],:pe[:, 1::2].shape[1]]\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(2)]\n",
    "    \n",
    "class PositionalEmbeddingV2(nn.Module):\n",
    "    def __init__(self, d_emb, seq_len):\n",
    "        super(PositionalEmbeddingV2, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        POS_EMB = torch.zeros(d_emb, seq_len).float()\n",
    "        POS_EMB.requires_grad=False\n",
    "\n",
    "        POS = torch.arange(0, seq_len).repeat(d_emb,1)\n",
    "        DIV = 10**(4*torch.arange(0, d_emb).reshape(-1,1).repeat(1,seq_len)*2/d_emb)\n",
    "\n",
    "        POS_EMB[0::2, :] = torch.sin(POS/DIV)[0::2, :]\n",
    "        POS_EMB[1::2, :] = torch.cos(POS/DIV)[1::2, :]\n",
    "        self.register_buffer('POS_EMB', POS_EMB)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.POS_EMB\n",
    "\n",
    "    \n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_emb):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_emb, \n",
    "                                    kernel_size=3, padding=1)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight,mode='fan_in',nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.tokenConv(x.permute(0, 2, 1)).transpose(1,2)\n",
    "        x = self.tokenConv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_k,d_v,d_emb ) -> None:\n",
    "        super(Attention, self).__init__()\n",
    "        self.d_emb = d_emb\n",
    "        self.W_query = nn.Parameter(torch.rand(d_k,d_emb))\n",
    "        self.W_keys = nn.Parameter(torch.rand(d_k,d_emb))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_v,d_emb))\n",
    "\n",
    "    def context_mat(self,query, keys):\n",
    "        # scaled dot product of queries and keys\n",
    "        Q_K = query.matmul(keys.permute(0,2,1))/np.sqrt(self.d_emb)\n",
    "        # row-wise softmax\n",
    "        sfmax_Q_K = torch.softmax(Q_K, dim=1)\n",
    "\n",
    "        return sfmax_Q_K\n",
    "\n",
    "    def forward(self,embed):\n",
    "\n",
    "        query = embed.matmul(self.W_query.T) \n",
    "        keys = embed.matmul(self.W_keys.T)\n",
    "        values = embed.matmul(self.W_value.T)\n",
    "        \n",
    "        sfmax_Q_K = self.context_mat(query=query, keys=keys)\n",
    "\n",
    "        att_out = sfmax_Q_K.matmul(values).contiguous()\n",
    "\n",
    "        return att_out, sfmax_Q_K\n",
    "    \n",
    "\n",
    "class AttentionForecastModel(nn.Module):\n",
    "    def __init__(self, d_k,d_v,d_emb, batch_size, seq_len, out_seq_len, c_in=1) -> None:\n",
    "        super(AttentionForecastModel, self).__init__()\n",
    "        self.dim_emb = d_emb\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len=seq_len\n",
    "        self.c_in = c_in\n",
    "        self.out_seq_len = out_seq_len\n",
    "\n",
    "        self.pos_emb = PositionalEmbeddingV2(seq_len=seq_len,d_emb=d_emb)\n",
    "        self.token_emb = TokenEmbedding(c_in=c_in, d_emb=d_emb)\n",
    "        self.attention = Attention(d_emb=d_emb,d_k=d_k, d_v=d_v)\n",
    "        self.linear_layer = nn.Linear(in_features=seq_len*d_v, out_features=out_seq_len)\n",
    "\n",
    "        self.sfmax_Q_K = None\n",
    "\n",
    "    def embed_output(self,input):\n",
    "        input_tensor = torch.tensor(input.reshape((self.batch_size,self.c_in,self.seq_len)), \n",
    "                                    dtype=torch.float32)\n",
    "        token_emb_out = self.token_emb(input_tensor)\n",
    "        pos_emb_out = self.pos_emb()\n",
    "\n",
    "        # changing dimensions while keeping the batch size dim the same\n",
    "        embed_tensor = token_emb_out+pos_emb_out\n",
    "        return embed_tensor.permute(0,2,1)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        embed_tensor = self.embed_output(input)\n",
    "        attn_out, sfmax_Q_K=self.attention(embed_tensor)\n",
    "        self.sfmax_Q_K = sfmax_Q_K\n",
    "        final_output = self.linear_layer(attn_out.reshape((self.batch_size,-1)))\n",
    "\n",
    "        return final_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 14, 3])\n"
     ]
    }
   ],
   "source": [
    "# # FOR DEBUGGING\n",
    "# EPOCHS = 50\n",
    "# OUT_VAR = 5\n",
    "# BATCH_SIZE = 10\n",
    "# feat_cols = all_cols[OUT_VAR:-1]\n",
    "# tgt_cols = all_cols[:OUT_VAR]\n",
    "\n",
    "# train_len = int(row_count*0.8)\n",
    "# batch_indexes = get_batch_indexes(batch_size=BATCH_SIZE, data_len=row_count, train_len=train_len)\n",
    "# st_index,end_index = batch_indexes[0]\n",
    "\n",
    "# Model = AttentionForecastModel(d_k=4, \n",
    "#                                d_v=3, \n",
    "#                                d_emb=5, \n",
    "#                                batch_size=BATCH_SIZE,\n",
    "#                                seq_len=len(feat_cols), \n",
    "#                                out_seq_len=len(tgt_cols))\n",
    "# batch_data = main_df.iloc[st_index:end_index]\n",
    "# feat_data = batch_data[feat_cols].to_numpy()\n",
    "# # tgt_data = torch.tensor(batch_data[tgt_cols].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "# embed = Model.embed_output(feat_data)\n",
    "# attn_out, sfmax_Q_K = Model.attention(embed)\n",
    "# print(attn_out.shape)\n",
    "\n",
    "\n",
    "# input_tnsr = torch.tensor(feat_data.reshape((Model.batch_size,Model.c_in,Model.seq_len)), \n",
    "#                                     dtype=torch.float32)\n",
    "# print(input_tnsr.shape)\n",
    "# Model.token_emb(input_tnsr).shape\n",
    "\n",
    "\n",
    "# tst_conv_layer = nn.Conv1d(in_channels=3, out_channels=1, \n",
    "#                                     kernel_size=3, padding=1)\n",
    "\n",
    "# tst_conv_layer(attn_out.permute(0,2,1)).shape\n",
    "\n",
    "# Model(feat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train cost: 88.97503935098644\n",
      "train cost: 17.081099903583528\n",
      "train cost: 11.454840859770778\n",
      "train cost: 6.718141293525697\n",
      "train cost: 4.523226238042117\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "OUT_VAR = 5\n",
    "BATCH_SIZE = 10\n",
    "feat_cols = all_cols[OUT_VAR:-1]\n",
    "tgt_cols = all_cols[:OUT_VAR]\n",
    "\n",
    "train_len = int(row_count*0.8)\n",
    "batch_indexes = get_batch_indexes(batch_size=BATCH_SIZE, data_len=row_count, train_len=train_len)\n",
    "\n",
    "Model = AttentionForecastModel(d_k=4, \n",
    "                               d_v=3, \n",
    "                               d_emb=5, \n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               seq_len=len(feat_cols), \n",
    "                               out_seq_len=len(tgt_cols))\n",
    "optimizer = torch.optim.Adam(Model.parameters(), lr = 0.01)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epch_trn_cost = 0\n",
    "    for st_index, end_index in batch_indexes:\n",
    "        batch_data = main_df.iloc[st_index:end_index]\n",
    "        feat_data = batch_data[feat_cols].to_numpy()\n",
    "        tgt_data = torch.tensor(batch_data[tgt_cols].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "        pred_out = Model(feat_data)\n",
    "        cost = nn.functional.huber_loss(pred_out,tgt_data)\n",
    "        epch_trn_cost+=(float(cost.detach())/BATCH_SIZE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch%10==0:\n",
    "        print('train cost:',epch_trn_cost )\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = main_df.iloc[800:]\n",
    "\n",
    "test_feat_data = test_data[feat_cols].to_numpy()\n",
    "test_tgt_data = test_data[tgt_cols].to_numpy()\n",
    "\n",
    "Model.batch_size= row_count-train_len\n",
    "test_pred = Model(test_feat_data).detach().numpy()\n",
    "\n",
    "test_pred_data = pd.DataFrame(np.round(test_pred,2))\n",
    "test_pred_data.columns = [f'pred_x{i}' for i in range(OUT_VAR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test_data =pd.concat([test_data.reset_index(drop=True),test_pred_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_col</th>\n",
       "      <th>SMAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pred_x0</td>\n",
       "      <td>0.084342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pred_x1</td>\n",
       "      <td>0.082287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pred_x2</td>\n",
       "      <td>0.078588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pred_x3</td>\n",
       "      <td>0.084798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pred_x4</td>\n",
       "      <td>0.084006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pred_col     SMAPE\n",
       "0  pred_x0  0.084342\n",
       "1  pred_x1  0.082287\n",
       "2  pred_x2  0.078588\n",
       "3  pred_x3  0.084798\n",
       "4  pred_x4  0.084006"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = {'pred_col':[],'SMAPE':[]}\n",
    "for i in range(OUT_VAR):\n",
    "    pred_col = f'pred_x{i}'\n",
    "    smape= mean_absolute_percentage_error(y_pred=combined_test_data[pred_col],\n",
    "                               y_true=combined_test_data[f'x{i}'],\n",
    "                               symmetric=True)\n",
    "    metrics_df['pred_col'].append(pred_col)\n",
    "    metrics_df['SMAPE'].append(smape)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_df)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>pred_x0</th>\n",
       "      <th>pred_x1</th>\n",
       "      <th>pred_x2</th>\n",
       "      <th>pred_x3</th>\n",
       "      <th>pred_x4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>146.02</td>\n",
       "      <td>96.70</td>\n",
       "      <td>66.12</td>\n",
       "      <td>46.81</td>\n",
       "      <td>34.23</td>\n",
       "      <td>25.88</td>\n",
       "      <td>20.14</td>\n",
       "      <td>16.11</td>\n",
       "      <td>13.26</td>\n",
       "      <td>11.19</td>\n",
       "      <td>...</td>\n",
       "      <td>6.44</td>\n",
       "      <td>6.22</td>\n",
       "      <td>6.08</td>\n",
       "      <td>6.02</td>\n",
       "      <td>6.0</td>\n",
       "      <td>148.289993</td>\n",
       "      <td>97.459999</td>\n",
       "      <td>67.349998</td>\n",
       "      <td>46.480000</td>\n",
       "      <td>33.900002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109.97</td>\n",
       "      <td>72.35</td>\n",
       "      <td>49.22</td>\n",
       "      <td>34.54</td>\n",
       "      <td>25.03</td>\n",
       "      <td>18.68</td>\n",
       "      <td>14.34</td>\n",
       "      <td>11.31</td>\n",
       "      <td>9.18</td>\n",
       "      <td>7.67</td>\n",
       "      <td>...</td>\n",
       "      <td>4.28</td>\n",
       "      <td>4.15</td>\n",
       "      <td>4.07</td>\n",
       "      <td>4.02</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.330002</td>\n",
       "      <td>72.910004</td>\n",
       "      <td>49.900002</td>\n",
       "      <td>34.189999</td>\n",
       "      <td>24.700001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104.29</td>\n",
       "      <td>68.84</td>\n",
       "      <td>46.99</td>\n",
       "      <td>33.15</td>\n",
       "      <td>24.11</td>\n",
       "      <td>18.06</td>\n",
       "      <td>13.97</td>\n",
       "      <td>11.13</td>\n",
       "      <td>9.12</td>\n",
       "      <td>7.68</td>\n",
       "      <td>...</td>\n",
       "      <td>4.32</td>\n",
       "      <td>4.17</td>\n",
       "      <td>4.08</td>\n",
       "      <td>4.02</td>\n",
       "      <td>4.0</td>\n",
       "      <td>105.559998</td>\n",
       "      <td>69.269997</td>\n",
       "      <td>47.630001</td>\n",
       "      <td>32.740002</td>\n",
       "      <td>23.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>197.74</td>\n",
       "      <td>130.52</td>\n",
       "      <td>89.09</td>\n",
       "      <td>62.74</td>\n",
       "      <td>45.71</td>\n",
       "      <td>34.43</td>\n",
       "      <td>26.69</td>\n",
       "      <td>21.35</td>\n",
       "      <td>17.57</td>\n",
       "      <td>14.89</td>\n",
       "      <td>...</td>\n",
       "      <td>8.55</td>\n",
       "      <td>8.28</td>\n",
       "      <td>8.12</td>\n",
       "      <td>8.04</td>\n",
       "      <td>8.0</td>\n",
       "      <td>196.369995</td>\n",
       "      <td>129.059998</td>\n",
       "      <td>89.250000</td>\n",
       "      <td>61.660000</td>\n",
       "      <td>44.990002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>25.34</td>\n",
       "      <td>16.78</td>\n",
       "      <td>11.51</td>\n",
       "      <td>8.15</td>\n",
       "      <td>5.95</td>\n",
       "      <td>4.47</td>\n",
       "      <td>3.46</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1.90</td>\n",
       "      <td>...</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.969999</td>\n",
       "      <td>17.160000</td>\n",
       "      <td>11.720000</td>\n",
       "      <td>8.300000</td>\n",
       "      <td>6.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>133.07</td>\n",
       "      <td>86.83</td>\n",
       "      <td>58.57</td>\n",
       "      <td>40.89</td>\n",
       "      <td>29.52</td>\n",
       "      <td>22.03</td>\n",
       "      <td>16.95</td>\n",
       "      <td>13.45</td>\n",
       "      <td>10.98</td>\n",
       "      <td>9.23</td>\n",
       "      <td>...</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.13</td>\n",
       "      <td>5.04</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>128.399994</td>\n",
       "      <td>84.239998</td>\n",
       "      <td>57.939999</td>\n",
       "      <td>39.840000</td>\n",
       "      <td>28.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>263.14</td>\n",
       "      <td>171.15</td>\n",
       "      <td>115.06</td>\n",
       "      <td>80.04</td>\n",
       "      <td>57.58</td>\n",
       "      <td>42.73</td>\n",
       "      <td>32.68</td>\n",
       "      <td>25.78</td>\n",
       "      <td>20.92</td>\n",
       "      <td>17.43</td>\n",
       "      <td>...</td>\n",
       "      <td>9.55</td>\n",
       "      <td>9.25</td>\n",
       "      <td>9.07</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>254.699997</td>\n",
       "      <td>166.690002</td>\n",
       "      <td>114.080002</td>\n",
       "      <td>78.320000</td>\n",
       "      <td>56.509998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>52.24</td>\n",
       "      <td>34.31</td>\n",
       "      <td>23.30</td>\n",
       "      <td>16.38</td>\n",
       "      <td>11.87</td>\n",
       "      <td>8.86</td>\n",
       "      <td>6.83</td>\n",
       "      <td>5.43</td>\n",
       "      <td>4.44</td>\n",
       "      <td>3.73</td>\n",
       "      <td>...</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>51.750000</td>\n",
       "      <td>33.980000</td>\n",
       "      <td>23.280001</td>\n",
       "      <td>16.090000</td>\n",
       "      <td>11.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>173.35</td>\n",
       "      <td>114.61</td>\n",
       "      <td>78.23</td>\n",
       "      <td>55.19</td>\n",
       "      <td>40.14</td>\n",
       "      <td>30.07</td>\n",
       "      <td>23.22</td>\n",
       "      <td>18.50</td>\n",
       "      <td>15.16</td>\n",
       "      <td>12.77</td>\n",
       "      <td>...</td>\n",
       "      <td>7.41</td>\n",
       "      <td>7.19</td>\n",
       "      <td>7.05</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>172.809998</td>\n",
       "      <td>113.489998</td>\n",
       "      <td>78.330002</td>\n",
       "      <td>54.020000</td>\n",
       "      <td>39.349998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0      x1      x2     x3     x4     x5     x6     x7     x8     x9  \\\n",
       "0      0.00    0.00    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "1    146.02   96.70   66.12  46.81  34.23  25.88  20.14  16.11  13.26  11.19   \n",
       "2    109.97   72.35   49.22  34.54  25.03  18.68  14.34  11.31   9.18   7.67   \n",
       "3    104.29   68.84   46.99  33.15  24.11  18.06  13.97  11.13   9.12   7.68   \n",
       "4    197.74  130.52   89.09  62.74  45.71  34.43  26.69  21.35  17.57  14.89   \n",
       "..      ...     ...     ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "195   25.34   16.78   11.51   8.15   5.95   4.47   3.46   2.76   2.26   1.90   \n",
       "196  133.07   86.83   58.57  40.89  29.52  22.03  16.95  13.45  10.98   9.23   \n",
       "197  263.14  171.15  115.06  80.04  57.58  42.73  32.68  25.78  20.92  17.43   \n",
       "198   52.24   34.31   23.30  16.38  11.87   8.86   6.83   5.43   4.44   3.73   \n",
       "199  173.35  114.61   78.23  55.19  40.14  30.07  23.22  18.50  15.16  12.77   \n",
       "\n",
       "     ...   x15   x16   x17   x18  x19     pred_x0     pred_x1     pred_x2  \\\n",
       "0    ...  0.00  0.00  0.00  0.00  0.0    0.490000    0.260000    0.300000   \n",
       "1    ...  6.44  6.22  6.08  6.02  6.0  148.289993   97.459999   67.349998   \n",
       "2    ...  4.28  4.15  4.07  4.02  4.0  111.330002   72.910004   49.900002   \n",
       "3    ...  4.32  4.17  4.08  4.02  4.0  105.559998   69.269997   47.630001   \n",
       "4    ...  8.55  8.28  8.12  8.04  8.0  196.369995  129.059998   89.250000   \n",
       "..   ...   ...   ...   ...   ...  ...         ...         ...         ...   \n",
       "195  ...  1.07  1.03  1.01  1.00  1.0   25.969999   17.160000   11.720000   \n",
       "196  ...  5.28  5.13  5.04  5.00  5.0  128.399994   84.239998   57.939999   \n",
       "197  ...  9.55  9.25  9.07  9.00  9.0  254.699997  166.690002  114.080002   \n",
       "198  ...  2.13  2.06  2.02  2.00  2.0   51.750000   33.980000   23.280001   \n",
       "199  ...  7.41  7.19  7.05  7.00  7.0  172.809998  113.489998   78.330002   \n",
       "\n",
       "       pred_x3    pred_x4  \n",
       "0     0.260000   0.380000  \n",
       "1    46.480000  33.900002  \n",
       "2    34.189999  24.700001  \n",
       "3    32.740002  23.770000  \n",
       "4    61.660000  44.990002  \n",
       "..         ...        ...  \n",
       "195   8.300000   6.020000  \n",
       "196  39.840000  28.930000  \n",
       "197  78.320000  56.509998  \n",
       "198  16.090000  11.740000  \n",
       "199  54.020000  39.349998  \n",
       "\n",
       "[200 rows x 25 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attention_out = Model.sfmax_Q_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(test_attention_out[9].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 10\n",
    "# input = main_df[all_cols].iloc[:batch_size].to_numpy().reshape((batch_size,1,7))\n",
    "\n",
    "# tok_emb = TokenEmbedding(c_in=1, d_emb=5)\n",
    "# output_tokn = tok_emb(torch.tensor(input, dtype=torch.float32))\n",
    "\n",
    "# pos_emb = PositionalEmbedding(d_emb=5)\n",
    "\n",
    "# output_pos = pos_emb(torch.tensor(input))\n",
    "\n",
    "# output_emb = output_tokn.permute(0,2,1)+output_pos\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

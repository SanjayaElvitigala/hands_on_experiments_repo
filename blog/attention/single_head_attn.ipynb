{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import copy, deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from sktime.performance_metrics.forecasting import (mean_absolute_percentage_error,\n",
    "                                                     mean_absolute_error)\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the synthetic data\n",
    "row_count = 1000\n",
    "n_var = 20\n",
    "\n",
    "if n_var<=10:\n",
    "    step_val = 0.1/n_var\n",
    "else:\n",
    "    step_val = 0.05/n_var\n",
    "\n",
    "start_var = np.round(np.random.rand(row_count)*10,0).tolist()\n",
    "data_dct = {}\n",
    "\n",
    "for x_var_num in range(n_var):\n",
    "    data_dct[f'x{x_var_num}'] = []\n",
    "data_dct[f'x{n_var-1}'] = start_var\n",
    "\n",
    "all_cols = list(data_dct.keys())\n",
    "\n",
    "multiply_factors = np.arange(start=1, stop=2, step=step_val)\n",
    "\n",
    "# an implementation to produce cumulatively increasing array of values row wise\n",
    "for row_num in range(row_count):\n",
    "    factor_cpy = copy(multiply_factors)\n",
    "    for index, x_var_num in enumerate(range(n_var-2,-1,-1)):\n",
    "        # getting a window of factors as per the x_var_num\n",
    "        windowed_factors = factor_cpy[index:index+3]\n",
    "        last_used_factor = np.random.choice(a=windowed_factors,size=1)[0]\n",
    "\n",
    "        calc_value = np.round(data_dct[f'x{x_var_num+1}'][row_num]*last_used_factor,2)\n",
    "        \n",
    "        data_dct[f'x{x_var_num}'].append(calc_value)\n",
    "\n",
    "        factor_cpy = factor_cpy[factor_cpy>last_used_factor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134.17</td>\n",
       "      <td>87.98</td>\n",
       "      <td>59.65</td>\n",
       "      <td>41.71</td>\n",
       "      <td>30.06</td>\n",
       "      <td>22.39</td>\n",
       "      <td>17.22</td>\n",
       "      <td>13.67</td>\n",
       "      <td>11.16</td>\n",
       "      <td>9.38</td>\n",
       "      <td>8.10</td>\n",
       "      <td>7.18</td>\n",
       "      <td>6.50</td>\n",
       "      <td>5.99</td>\n",
       "      <td>5.61</td>\n",
       "      <td>5.34</td>\n",
       "      <td>5.17</td>\n",
       "      <td>5.06</td>\n",
       "      <td>5.01</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163.88</td>\n",
       "      <td>108.53</td>\n",
       "      <td>74.21</td>\n",
       "      <td>52.54</td>\n",
       "      <td>38.42</td>\n",
       "      <td>28.94</td>\n",
       "      <td>22.52</td>\n",
       "      <td>18.02</td>\n",
       "      <td>14.80</td>\n",
       "      <td>12.49</td>\n",
       "      <td>10.81</td>\n",
       "      <td>9.59</td>\n",
       "      <td>8.72</td>\n",
       "      <td>8.11</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.36</td>\n",
       "      <td>7.16</td>\n",
       "      <td>7.05</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131.15</td>\n",
       "      <td>87.58</td>\n",
       "      <td>60.40</td>\n",
       "      <td>42.99</td>\n",
       "      <td>31.55</td>\n",
       "      <td>23.90</td>\n",
       "      <td>18.71</td>\n",
       "      <td>15.06</td>\n",
       "      <td>12.45</td>\n",
       "      <td>10.55</td>\n",
       "      <td>9.17</td>\n",
       "      <td>8.19</td>\n",
       "      <td>7.46</td>\n",
       "      <td>6.94</td>\n",
       "      <td>6.56</td>\n",
       "      <td>6.31</td>\n",
       "      <td>6.16</td>\n",
       "      <td>6.07</td>\n",
       "      <td>6.02</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x0      x1     x2     x3     x4     x5     x6     x7     x8     x9  \\\n",
       "0  134.17   87.98  59.65  41.71  30.06  22.39  17.22  13.67  11.16   9.38   \n",
       "1  163.88  108.53  74.21  52.54  38.42  28.94  22.52  18.02  14.80  12.49   \n",
       "2  131.15   87.58  60.40  42.99  31.55  23.90  18.71  15.06  12.45  10.55   \n",
       "\n",
       "     x10   x11   x12   x13   x14   x15   x16   x17   x18  x19  \n",
       "0   8.10  7.18  6.50  5.99  5.61  5.34  5.17  5.06  5.01  5.0  \n",
       "1  10.81  9.59  8.72  8.11  7.67  7.36  7.16  7.05  7.00  7.0  \n",
       "2   9.17  8.19  7.46  6.94  6.56  6.31  6.16  6.07  6.02  6.0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = pd.DataFrame(data_dct)\n",
    "main_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_diff_data = False\n",
    "\n",
    "if is_diff_data:\n",
    "    main_df = pd.DataFrame(np.round(main_df[all_cols[:-1]].to_numpy()-main_df[all_cols[1:]].to_numpy(),3), columns=all_cols[:-1])\n",
    "    n_var = len(main_df.columns)\n",
    "    all_cols = list(main_df.columns)\n",
    "    main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_indexes(batch_size, data_len, train_len):\n",
    "    total_batches = train_len//batch_size\n",
    "\n",
    "    batch_indexes = np.column_stack((np.arange(start=0,stop=data_len, step=batch_size),\n",
    "               np.arange(start=0+batch_size,stop=data_len+batch_size, step=batch_size)))\n",
    "    return batch_indexes[:total_batches]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_emb, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_emb).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_emb, 2).float() * -(math.log(10000.0) / d_emb)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)[:pe[:, 0::2].shape[0],:pe[:, 0::2].shape[1]]\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)[:pe[:, 1::2].shape[0],:pe[:, 1::2].shape[1]]\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(2)]\n",
    "    \n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_emb):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_emb, \n",
    "                                    kernel_size=3, padding=1)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight,mode='fan_in',nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.tokenConv(x.permute(0, 2, 1)).transpose(1,2)\n",
    "        x = self.tokenConv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_k,d_v,d_emb ) -> None:\n",
    "        super(Attention, self).__init__()\n",
    "        self.d_emb = d_emb\n",
    "        self.W_query = nn.Parameter(torch.rand(d_k,d_emb))\n",
    "        self.W_keys = nn.Parameter(torch.rand(d_k,d_emb))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_v,d_emb))\n",
    "\n",
    "    def context_mat(self,query, keys):\n",
    "        # scaled dot product of queries and keys\n",
    "        Q_K = query.matmul(keys.permute(0,2,1))/np.sqrt(self.d_emb)\n",
    "        # row-wise softmax\n",
    "        sfmax_Q_K = torch.softmax(Q_K, dim=1)\n",
    "\n",
    "        return sfmax_Q_K\n",
    "\n",
    "    def forward(self,embed):\n",
    "\n",
    "        query = embed.matmul(self.W_query.T) \n",
    "        keys = embed.matmul(self.W_keys.T)\n",
    "        values = embed.matmul(self.W_value.T)\n",
    "        \n",
    "        sfmax_Q_K = self.context_mat(query=query, keys=keys)\n",
    "\n",
    "        att_out = sfmax_Q_K.matmul(values).contiguous()\n",
    "\n",
    "        return att_out, sfmax_Q_K\n",
    "    \n",
    "\n",
    "class AttentionForecastModel(nn.Module):\n",
    "    def __init__(self, d_k,d_v,d_emb, batch_size, seq_len, out_seq_len, c_in=1) -> None:\n",
    "        super(AttentionForecastModel, self).__init__()\n",
    "        self.dim_emb = d_emb\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len=seq_len\n",
    "        self.c_in = c_in\n",
    "        self.out_seq_len = out_seq_len\n",
    "\n",
    "        self.pos_emb = PositionalEmbedding(d_emb=d_emb)\n",
    "        self.token_emb = TokenEmbedding(c_in=c_in, d_emb=d_emb)\n",
    "        self.attention = Attention(d_emb=d_emb,d_k=d_k, d_v=d_v)\n",
    "        self.linear_layer = nn.Linear(in_features=seq_len*d_v, out_features=out_seq_len)\n",
    "\n",
    "        self.sfmax_Q_K = None\n",
    "\n",
    "    def embed_output(self,input):\n",
    "        input_tensor = torch.tensor(input.reshape((self.batch_size,self.c_in,self.seq_len)), \n",
    "                                    dtype=torch.float32)\n",
    "        token_emb_out = self.token_emb(input_tensor)\n",
    "        pos_emb_out = self.pos_emb(input_tensor)\n",
    "\n",
    "        # changing dimensions while keeping the batch size dim the same\n",
    "        embed_tensor = token_emb_out.permute(0,2,1)+pos_emb_out\n",
    "        return embed_tensor\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        embed_tensor = self.embed_output(input)\n",
    "        attn_out, sfmax_Q_K=self.attention(embed_tensor)\n",
    "        self.sfmax_Q_K = sfmax_Q_K\n",
    "        final_output = self.linear_layer(attn_out.reshape((self.batch_size,-1)))\n",
    "\n",
    "        return final_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 10\n",
    "# out_var = 2\n",
    "# input_var = n_var - out_var\n",
    "# input = main_df[sorted(all_cols, reverse=True)[:input_var]].iloc[:batch_size].to_numpy()\n",
    "# Model = AttentionForecastModel(d_k=4, \n",
    "#                                d_v=3, \n",
    "#                                d_emb=5, \n",
    "#                                batch_size=batch_size,\n",
    "#                                seq_len=input_var, \n",
    "#                                out_seq_len=out_var)\n",
    "# Model(input)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train cost: 93.67657330036162\n",
      "train cost: 17.84588124155999\n",
      "train cost: 15.181572574377059\n",
      "train cost: 12.983339509367944\n",
      "train cost: 10.333689454197884\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "OUT_VAR = 5\n",
    "BATCH_SIZE = 10\n",
    "feat_cols = all_cols[OUT_VAR:-1]\n",
    "tgt_cols = all_cols[:OUT_VAR]\n",
    "\n",
    "train_len = int(row_count*0.8)\n",
    "batch_indexes = get_batch_indexes(batch_size=BATCH_SIZE, data_len=row_count, train_len=train_len)\n",
    "\n",
    "Model = AttentionForecastModel(d_k=4, \n",
    "                               d_v=3, \n",
    "                               d_emb=5, \n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               seq_len=len(feat_cols), \n",
    "                               out_seq_len=len(tgt_cols))\n",
    "optimizer = torch.optim.Adam(Model.parameters(), lr = 0.01)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epch_trn_cost = 0\n",
    "    for st_index, end_index in batch_indexes:\n",
    "        batch_data = main_df.iloc[st_index:end_index]\n",
    "        feat_data = batch_data[feat_cols].to_numpy()\n",
    "        tgt_data = torch.tensor(batch_data[tgt_cols].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "        pred_out = Model(feat_data)\n",
    "        cost = nn.functional.huber_loss(pred_out,tgt_data)\n",
    "        epch_trn_cost+=(float(cost.detach())/BATCH_SIZE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch%10==0:\n",
    "        print('train cost:',epch_trn_cost )\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = main_df.iloc[800:]\n",
    "\n",
    "test_feat_data = test_data[feat_cols].to_numpy()\n",
    "test_tgt_data = test_data[tgt_cols].to_numpy()\n",
    "\n",
    "Model.batch_size= row_count-train_len\n",
    "test_pred = Model(test_feat_data).detach().numpy()\n",
    "\n",
    "test_pred_data = pd.DataFrame(np.round(test_pred,2))\n",
    "test_pred_data.columns = [f'pred_x{i}' for i in range(OUT_VAR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test_data =pd.concat([test_data.reset_index(drop=True),test_pred_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_col</th>\n",
       "      <th>SMAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pred_x0</td>\n",
       "      <td>0.091104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pred_x1</td>\n",
       "      <td>0.086069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pred_x2</td>\n",
       "      <td>0.091154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pred_x3</td>\n",
       "      <td>0.099329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pred_x4</td>\n",
       "      <td>0.138366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pred_col     SMAPE\n",
       "0  pred_x0  0.091104\n",
       "1  pred_x1  0.086069\n",
       "2  pred_x2  0.091154\n",
       "3  pred_x3  0.099329\n",
       "4  pred_x4  0.138366"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = {'pred_col':[],'SMAPE':[]}\n",
    "for i in range(OUT_VAR):\n",
    "    pred_col = f'pred_x{i}'\n",
    "    smape= mean_absolute_percentage_error(y_pred=combined_test_data[pred_col],\n",
    "                               y_true=combined_test_data[f'x{i}'],\n",
    "                               symmetric=True)\n",
    "    metrics_df['pred_col'].append(pred_col)\n",
    "    metrics_df['SMAPE'].append(smape)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_df)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>pred_x0</th>\n",
       "      <th>pred_x1</th>\n",
       "      <th>pred_x2</th>\n",
       "      <th>pred_x3</th>\n",
       "      <th>pred_x4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>-0.120000</td>\n",
       "      <td>-0.140000</td>\n",
       "      <td>-0.130000</td>\n",
       "      <td>-0.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>146.02</td>\n",
       "      <td>96.70</td>\n",
       "      <td>66.12</td>\n",
       "      <td>46.81</td>\n",
       "      <td>34.23</td>\n",
       "      <td>25.88</td>\n",
       "      <td>20.14</td>\n",
       "      <td>16.11</td>\n",
       "      <td>13.26</td>\n",
       "      <td>11.19</td>\n",
       "      <td>...</td>\n",
       "      <td>6.44</td>\n",
       "      <td>6.22</td>\n",
       "      <td>6.08</td>\n",
       "      <td>6.02</td>\n",
       "      <td>6.0</td>\n",
       "      <td>149.970001</td>\n",
       "      <td>99.260002</td>\n",
       "      <td>66.750000</td>\n",
       "      <td>46.650002</td>\n",
       "      <td>32.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109.97</td>\n",
       "      <td>72.35</td>\n",
       "      <td>49.22</td>\n",
       "      <td>34.54</td>\n",
       "      <td>25.03</td>\n",
       "      <td>18.68</td>\n",
       "      <td>14.34</td>\n",
       "      <td>11.31</td>\n",
       "      <td>9.18</td>\n",
       "      <td>7.67</td>\n",
       "      <td>...</td>\n",
       "      <td>4.28</td>\n",
       "      <td>4.15</td>\n",
       "      <td>4.07</td>\n",
       "      <td>4.02</td>\n",
       "      <td>4.0</td>\n",
       "      <td>108.250000</td>\n",
       "      <td>71.540001</td>\n",
       "      <td>47.919998</td>\n",
       "      <td>33.240002</td>\n",
       "      <td>23.110001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104.29</td>\n",
       "      <td>68.84</td>\n",
       "      <td>46.99</td>\n",
       "      <td>33.15</td>\n",
       "      <td>24.11</td>\n",
       "      <td>18.06</td>\n",
       "      <td>13.97</td>\n",
       "      <td>11.13</td>\n",
       "      <td>9.12</td>\n",
       "      <td>7.68</td>\n",
       "      <td>...</td>\n",
       "      <td>4.32</td>\n",
       "      <td>4.17</td>\n",
       "      <td>4.08</td>\n",
       "      <td>4.02</td>\n",
       "      <td>4.0</td>\n",
       "      <td>105.199997</td>\n",
       "      <td>69.599998</td>\n",
       "      <td>46.750000</td>\n",
       "      <td>32.580002</td>\n",
       "      <td>22.690001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>197.74</td>\n",
       "      <td>130.52</td>\n",
       "      <td>89.09</td>\n",
       "      <td>62.74</td>\n",
       "      <td>45.71</td>\n",
       "      <td>34.43</td>\n",
       "      <td>26.69</td>\n",
       "      <td>21.35</td>\n",
       "      <td>17.57</td>\n",
       "      <td>14.89</td>\n",
       "      <td>...</td>\n",
       "      <td>8.55</td>\n",
       "      <td>8.28</td>\n",
       "      <td>8.12</td>\n",
       "      <td>8.04</td>\n",
       "      <td>8.0</td>\n",
       "      <td>198.850006</td>\n",
       "      <td>131.570007</td>\n",
       "      <td>88.449997</td>\n",
       "      <td>61.779999</td>\n",
       "      <td>43.150002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>25.34</td>\n",
       "      <td>16.78</td>\n",
       "      <td>11.51</td>\n",
       "      <td>8.15</td>\n",
       "      <td>5.95</td>\n",
       "      <td>4.47</td>\n",
       "      <td>3.46</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1.90</td>\n",
       "      <td>...</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.139999</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>8.060000</td>\n",
       "      <td>5.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>133.07</td>\n",
       "      <td>86.83</td>\n",
       "      <td>58.57</td>\n",
       "      <td>40.89</td>\n",
       "      <td>29.52</td>\n",
       "      <td>22.03</td>\n",
       "      <td>16.95</td>\n",
       "      <td>13.45</td>\n",
       "      <td>10.98</td>\n",
       "      <td>9.23</td>\n",
       "      <td>...</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.13</td>\n",
       "      <td>5.04</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>126.529999</td>\n",
       "      <td>83.599998</td>\n",
       "      <td>56.060001</td>\n",
       "      <td>39.009998</td>\n",
       "      <td>27.200001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>263.14</td>\n",
       "      <td>171.15</td>\n",
       "      <td>115.06</td>\n",
       "      <td>80.04</td>\n",
       "      <td>57.58</td>\n",
       "      <td>42.73</td>\n",
       "      <td>32.68</td>\n",
       "      <td>25.78</td>\n",
       "      <td>20.92</td>\n",
       "      <td>17.43</td>\n",
       "      <td>...</td>\n",
       "      <td>9.55</td>\n",
       "      <td>9.25</td>\n",
       "      <td>9.07</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>248.679993</td>\n",
       "      <td>164.440002</td>\n",
       "      <td>110.089996</td>\n",
       "      <td>76.190002</td>\n",
       "      <td>52.900002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>52.24</td>\n",
       "      <td>34.31</td>\n",
       "      <td>23.30</td>\n",
       "      <td>16.38</td>\n",
       "      <td>11.87</td>\n",
       "      <td>8.86</td>\n",
       "      <td>6.83</td>\n",
       "      <td>5.43</td>\n",
       "      <td>4.44</td>\n",
       "      <td>3.73</td>\n",
       "      <td>...</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>51.130001</td>\n",
       "      <td>33.779999</td>\n",
       "      <td>22.670000</td>\n",
       "      <td>15.820000</td>\n",
       "      <td>11.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>173.35</td>\n",
       "      <td>114.61</td>\n",
       "      <td>78.23</td>\n",
       "      <td>55.19</td>\n",
       "      <td>40.14</td>\n",
       "      <td>30.07</td>\n",
       "      <td>23.22</td>\n",
       "      <td>18.50</td>\n",
       "      <td>15.16</td>\n",
       "      <td>12.77</td>\n",
       "      <td>...</td>\n",
       "      <td>7.41</td>\n",
       "      <td>7.19</td>\n",
       "      <td>7.05</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>172.410004</td>\n",
       "      <td>113.949997</td>\n",
       "      <td>76.489998</td>\n",
       "      <td>53.340000</td>\n",
       "      <td>37.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0      x1      x2     x3     x4     x5     x6     x7     x8     x9  \\\n",
       "0      0.00    0.00    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "1    146.02   96.70   66.12  46.81  34.23  25.88  20.14  16.11  13.26  11.19   \n",
       "2    109.97   72.35   49.22  34.54  25.03  18.68  14.34  11.31   9.18   7.67   \n",
       "3    104.29   68.84   46.99  33.15  24.11  18.06  13.97  11.13   9.12   7.68   \n",
       "4    197.74  130.52   89.09  62.74  45.71  34.43  26.69  21.35  17.57  14.89   \n",
       "..      ...     ...     ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "195   25.34   16.78   11.51   8.15   5.95   4.47   3.46   2.76   2.26   1.90   \n",
       "196  133.07   86.83   58.57  40.89  29.52  22.03  16.95  13.45  10.98   9.23   \n",
       "197  263.14  171.15  115.06  80.04  57.58  42.73  32.68  25.78  20.92  17.43   \n",
       "198   52.24   34.31   23.30  16.38  11.87   8.86   6.83   5.43   4.44   3.73   \n",
       "199  173.35  114.61   78.23  55.19  40.14  30.07  23.22  18.50  15.16  12.77   \n",
       "\n",
       "     ...   x15   x16   x17   x18  x19     pred_x0     pred_x1     pred_x2  \\\n",
       "0    ...  0.00  0.00  0.00  0.00  0.0   -0.150000   -0.120000   -0.140000   \n",
       "1    ...  6.44  6.22  6.08  6.02  6.0  149.970001   99.260002   66.750000   \n",
       "2    ...  4.28  4.15  4.07  4.02  4.0  108.250000   71.540001   47.919998   \n",
       "3    ...  4.32  4.17  4.08  4.02  4.0  105.199997   69.599998   46.750000   \n",
       "4    ...  8.55  8.28  8.12  8.04  8.0  198.850006  131.570007   88.449997   \n",
       "..   ...   ...   ...   ...   ...  ...         ...         ...         ...   \n",
       "195  ...  1.07  1.03  1.01  1.00  1.0   26.139999   17.240000   11.580000   \n",
       "196  ...  5.28  5.13  5.04  5.00  5.0  126.529999   83.599998   56.060001   \n",
       "197  ...  9.55  9.25  9.07  9.00  9.0  248.679993  164.440002  110.089996   \n",
       "198  ...  2.13  2.06  2.02  2.00  2.0   51.130001   33.779999   22.670000   \n",
       "199  ...  7.41  7.19  7.05  7.00  7.0  172.410004  113.949997   76.489998   \n",
       "\n",
       "       pred_x3    pred_x4  \n",
       "0    -0.130000  -0.110000  \n",
       "1    46.650002  32.570000  \n",
       "2    33.240002  23.110001  \n",
       "3    32.580002  22.690001  \n",
       "4    61.779999  43.150002  \n",
       "..         ...        ...  \n",
       "195   8.060000   5.630000  \n",
       "196  39.009998  27.200001  \n",
       "197  76.190002  52.900002  \n",
       "198  15.820000  11.010000  \n",
       "199  53.340000  37.250000  \n",
       "\n",
       "[200 rows x 25 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attention_out = Model.sfmax_Q_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(test_attention_out[5].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 10\n",
    "# input = main_df[all_cols].iloc[:batch_size].to_numpy().reshape((batch_size,1,7))\n",
    "\n",
    "# tok_emb = TokenEmbedding(c_in=1, d_emb=5)\n",
    "# output_tokn = tok_emb(torch.tensor(input, dtype=torch.float32))\n",
    "\n",
    "# pos_emb = PositionalEmbedding(d_emb=5)\n",
    "\n",
    "# output_pos = pos_emb(torch.tensor(input))\n",
    "\n",
    "# output_emb = output_tokn.permute(0,2,1)+output_pos\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import copy, deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the synthetic data\n",
    "row_count = 500\n",
    "n_var = 7\n",
    "\n",
    "if n_var<=10:\n",
    "    step_val = 0.1/n_var\n",
    "else:\n",
    "    step_val = 0.05/n_var\n",
    "\n",
    "start_var = np.round(np.random.rand(row_count)*10,0).tolist()\n",
    "data_dct = {}\n",
    "\n",
    "for x_var_num in range(n_var):\n",
    "    data_dct[f'x{x_var_num}'] = []\n",
    "data_dct[f'x{n_var-1}'] = start_var\n",
    "\n",
    "all_cols = list(data_dct.keys())\n",
    "\n",
    "multiply_factors = np.arange(start=1, stop=2, step=step_val)\n",
    "\n",
    "# an implementation to produce cumulatively increasing array of values row wise\n",
    "for row_num in range(row_count):\n",
    "    factor_cpy = copy(multiply_factors)\n",
    "    for index, x_var_num in enumerate(range(n_var-2,-1,-1)):\n",
    "        # getting a window of factors as per the x_var_num\n",
    "        windowed_factors = factor_cpy[index:index+3]\n",
    "        last_used_factor = np.random.choice(a=windowed_factors,size=1)[0]\n",
    "\n",
    "        calc_value = np.round(data_dct[f'x{x_var_num+1}'][row_num]*last_used_factor,2)\n",
    "        \n",
    "        data_dct[f'x{x_var_num}'].append(calc_value)\n",
    "\n",
    "        factor_cpy = factor_cpy[factor_cpy>last_used_factor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.35</td>\n",
       "      <td>11.08</td>\n",
       "      <td>8.62</td>\n",
       "      <td>7.27</td>\n",
       "      <td>6.52</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.95</td>\n",
       "      <td>14.55</td>\n",
       "      <td>11.32</td>\n",
       "      <td>9.43</td>\n",
       "      <td>8.46</td>\n",
       "      <td>8.11</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.18</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x0     x1     x2    x3    x4    x5   x6\n",
       "0  15.35  11.08   8.62  7.27  6.52  6.09  6.0\n",
       "1  19.95  14.55  11.32  9.43  8.46  8.11  8.0\n",
       "2   2.18   1.62   1.30  1.12  1.03  1.00  1.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = pd.DataFrame(data_dct)\n",
    "main_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)[:pe[:, 0::2].shape[0],:pe[:, 0::2].shape[1]]\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)[:pe[:, 1::2].shape[0],:pe[:, 1::2].shape[1]]\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "    \n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model, \n",
    "                                    kernel_size=3, padding=1)\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv1d):\n",
    "        #         nn.init.kaiming_normal_(m.weight,mode='fan_in',nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.tokenConv(x.permute(0, 2, 1)).transpose(1,2)\n",
    "        x = self.tokenConv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15.3500, 11.0800,  8.6200,  7.2700,  6.5200,  6.0900,  6.0000]])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.tensor(input, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7)\n",
      "torch.Size([5, 7])\n"
     ]
    }
   ],
   "source": [
    "input = main_df[all_cols].iloc[:1].to_numpy()#.reshape(1,-1)\n",
    "\n",
    "tok_emb = TokenEmbedding(c_in=1, d_model=5)\n",
    "output_tokn = tok_emb(torch.tensor(input, dtype=torch.float32))\n",
    "\n",
    "print(input.shape)\n",
    "print(output_tokn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.8626, -5.3464, -4.1868, -3.5255, -3.1587, -2.9843, -1.7063],\n",
       "        [ 2.2977,  1.5117,  1.4736,  1.4535,  1.4379,  1.4744, -0.2722],\n",
       "        [-2.1218, -1.7537, -1.4172, -1.2306, -1.1258, -1.0805, -0.5122],\n",
       "        [-4.1864, -9.3352, -6.8143, -5.3852, -4.6026, -4.1126, -5.8567],\n",
       "        [ 1.7869, -7.5656, -5.6631, -4.5517, -3.9400, -3.6429, -1.8968]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  2.5116e-02,  9.9968e-01,  6.3096e-04],\n",
       "         [ 9.0930e-01, -4.1615e-01,  5.0217e-02,  9.9874e-01,  1.2619e-03],\n",
       "         [ 1.4112e-01, -9.8999e-01,  7.5285e-02,  9.9716e-01,  1.8929e-03],\n",
       "         [-7.5680e-01, -6.5364e-01,  1.0031e-01,  9.9496e-01,  2.5238e-03],\n",
       "         [-9.5892e-01,  2.8366e-01,  1.2526e-01,  9.9212e-01,  3.1548e-03],\n",
       "         [-2.7942e-01,  9.6017e-01,  1.5014e-01,  9.8866e-01,  3.7857e-03]]])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_emb = PositionalEmbedding(d_model=5)\n",
    "pos_emb(torch.tensor(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-12.0616,  -0.3440,  -2.7621,   4.2544, -10.7779],\n",
       "         [ -8.1451,   3.2002,   4.4332,  -4.2393,  -1.4906],\n",
       "         [ -6.4380,   1.7728,   3.0214,  -2.3095,  -1.8753],\n",
       "         [ -6.3022,   0.9117,   2.2089,  -1.1953,  -2.1326],\n",
       "         [ -6.6902,   1.0866,   1.7764,  -0.5923,  -2.2617],\n",
       "         [ -6.6890,   2.0011,   1.5408,  -0.1893,  -2.3993],\n",
       "         [ -2.5627,   0.1421,   1.7162,  -2.6869,   0.2941]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokn.T+pos_emb(torch.tensor(input))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

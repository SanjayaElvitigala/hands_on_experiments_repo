{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import copy, deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from sktime.performance_metrics.forecasting import (mean_absolute_percentage_error,\n",
    "                                                     mean_absolute_error)\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the synthetic data\n",
    "row_count = 1000\n",
    "n_var = 20\n",
    "\n",
    "if n_var<=10:\n",
    "    step_val = 0.1/n_var\n",
    "else:\n",
    "    step_val = 0.05/n_var\n",
    "\n",
    "start_var = np.round(np.random.rand(row_count)*10,0).tolist()\n",
    "data_dct = {}\n",
    "\n",
    "for x_var_num in range(n_var):\n",
    "    data_dct[f'x{x_var_num}'] = []\n",
    "data_dct[f'x{n_var-1}'] = start_var\n",
    "\n",
    "all_cols = list(data_dct.keys())\n",
    "\n",
    "multiply_factors = np.arange(start=1, stop=2, step=step_val)\n",
    "\n",
    "# an implementation to produce cumulatively increasing array of values row wise\n",
    "for row_num in range(row_count):\n",
    "    factor_cpy = copy(multiply_factors)\n",
    "    for index, x_var_num in enumerate(range(n_var-2,-1,-1)):\n",
    "        # getting a window of factors as per the x_var_num\n",
    "        windowed_factors = factor_cpy[index:index+3]\n",
    "        last_used_factor = np.random.choice(a=windowed_factors,size=1)[0]\n",
    "\n",
    "        calc_value = np.round(data_dct[f'x{x_var_num+1}'][row_num]*last_used_factor,2)\n",
    "        \n",
    "        data_dct[f'x{x_var_num}'].append(calc_value)\n",
    "\n",
    "        factor_cpy = factor_cpy[factor_cpy>last_used_factor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.72</td>\n",
       "      <td>16.26</td>\n",
       "      <td>11.08</td>\n",
       "      <td>7.82</td>\n",
       "      <td>5.71</td>\n",
       "      <td>4.30</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>264.95</td>\n",
       "      <td>173.17</td>\n",
       "      <td>116.81</td>\n",
       "      <td>81.54</td>\n",
       "      <td>58.66</td>\n",
       "      <td>43.61</td>\n",
       "      <td>33.35</td>\n",
       "      <td>26.31</td>\n",
       "      <td>21.35</td>\n",
       "      <td>17.83</td>\n",
       "      <td>15.24</td>\n",
       "      <td>13.37</td>\n",
       "      <td>11.96</td>\n",
       "      <td>10.95</td>\n",
       "      <td>10.23</td>\n",
       "      <td>9.74</td>\n",
       "      <td>9.41</td>\n",
       "      <td>9.18</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138.82</td>\n",
       "      <td>91.78</td>\n",
       "      <td>62.86</td>\n",
       "      <td>44.58</td>\n",
       "      <td>32.72</td>\n",
       "      <td>24.79</td>\n",
       "      <td>19.37</td>\n",
       "      <td>15.56</td>\n",
       "      <td>12.86</td>\n",
       "      <td>10.90</td>\n",
       "      <td>9.48</td>\n",
       "      <td>8.43</td>\n",
       "      <td>7.65</td>\n",
       "      <td>7.10</td>\n",
       "      <td>6.70</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.21</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.03</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x0      x1      x2     x3     x4     x5     x6     x7     x8     x9  \\\n",
       "0   24.72   16.26   11.08   7.82   5.71   4.30   3.33   2.66   2.18   1.84   \n",
       "1  264.95  173.17  116.81  81.54  58.66  43.61  33.35  26.31  21.35  17.83   \n",
       "2  138.82   91.78   62.86  44.58  32.72  24.79  19.37  15.56  12.86  10.90   \n",
       "\n",
       "     x10    x11    x12    x13    x14   x15   x16   x17   x18  x19  \n",
       "0   1.59   1.41   1.28   1.18   1.11  1.06  1.03  1.01  1.00  1.0  \n",
       "1  15.24  13.37  11.96  10.95  10.23  9.74  9.41  9.18  9.04  9.0  \n",
       "2   9.48   8.43   7.65   7.10   6.70  6.41  6.21  6.09  6.03  6.0  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = pd.DataFrame(data_dct)\n",
    "main_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_diff_data = True\n",
    "\n",
    "if is_diff_data:\n",
    "    main_df = pd.DataFrame(np.round(main_df[all_cols[:-1]].to_numpy()-main_df[all_cols[1:]].to_numpy(),3), columns=all_cols[:-1])\n",
    "    n_var = len(main_df.columns)\n",
    "    all_cols = list(main_df.columns)\n",
    "    main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_indexes(batch_size, data_len, train_len):\n",
    "    total_batches = train_len//batch_size\n",
    "\n",
    "    batch_indexes = np.column_stack((np.arange(start=0,stop=data_len, step=batch_size),\n",
    "               np.arange(start=0+batch_size,stop=data_len+batch_size, step=batch_size)))\n",
    "    return batch_indexes[:total_batches]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_emb, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_emb).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_emb, 2).float() * -(math.log(10000.0) / d_emb)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)[:pe[:, 0::2].shape[0],:pe[:, 0::2].shape[1]]\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)[:pe[:, 1::2].shape[0],:pe[:, 1::2].shape[1]]\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(2)]\n",
    "    \n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_emb):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_emb, \n",
    "                                    kernel_size=3, padding=1)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight,mode='fan_in',nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.tokenConv(x.permute(0, 2, 1)).transpose(1,2)\n",
    "        x = self.tokenConv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_k,d_v,d_emb ) -> None:\n",
    "        super(Attention, self).__init__()\n",
    "        self.d_emb = d_emb\n",
    "        self.W_query = nn.Parameter(torch.rand(d_k,d_emb))\n",
    "        self.W_keys = nn.Parameter(torch.rand(d_k,d_emb))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_v,d_emb))\n",
    "\n",
    "    def context_mat(self,query, keys):\n",
    "        # scaled dot product of queries and keys\n",
    "        Q_K = query.matmul(keys.permute(0,2,1))/np.sqrt(self.d_emb)\n",
    "        # row-wise softmax\n",
    "        sfmax_Q_K = torch.softmax(Q_K, dim=1)\n",
    "\n",
    "        return sfmax_Q_K\n",
    "\n",
    "    def forward(self,embed):\n",
    "\n",
    "        query = embed.matmul(self.W_query.T) \n",
    "        keys = embed.matmul(self.W_keys.T)\n",
    "        values = embed.matmul(self.W_value.T)\n",
    "        \n",
    "        sfmax_Q_K = self.context_mat(query=query, keys=keys)\n",
    "\n",
    "        att_out = sfmax_Q_K.matmul(values).contiguous()\n",
    "\n",
    "        return att_out, sfmax_Q_K\n",
    "    \n",
    "\n",
    "class AttentionForecastModel(nn.Module):\n",
    "    def __init__(self, d_k,d_v,d_emb, batch_size, seq_len, out_seq_len, c_in=1) -> None:\n",
    "        super(AttentionForecastModel, self).__init__()\n",
    "        self.dim_emb = d_emb\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len=seq_len\n",
    "        self.c_in = c_in\n",
    "        self.out_seq_len = out_seq_len\n",
    "\n",
    "        self.pos_emb = PositionalEmbedding(d_emb=d_emb)\n",
    "        self.token_emb = TokenEmbedding(c_in=c_in, d_emb=d_emb)\n",
    "        self.attention = Attention(d_emb=d_emb,d_k=d_k, d_v=d_v)\n",
    "        self.linear_layer = nn.Linear(in_features=seq_len*d_v, out_features=out_seq_len)\n",
    "\n",
    "        self.sfmax_Q_K = None\n",
    "\n",
    "    def embed_output(self,input):\n",
    "        input_tensor = torch.tensor(input.reshape((self.batch_size,self.c_in,self.seq_len)), \n",
    "                                    dtype=torch.float32)\n",
    "        token_emb_out = self.token_emb(input_tensor)\n",
    "        pos_emb_out = self.pos_emb(input_tensor)\n",
    "\n",
    "        # changing dimensions while keeping the batch size dim the same\n",
    "        embed_tensor = token_emb_out.permute(0,2,1)+pos_emb_out\n",
    "        return embed_tensor\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        embed_tensor = self.embed_output(input)\n",
    "        attn_out, sfmax_Q_K=self.attention(embed_tensor)\n",
    "        self.sfmax_Q_K = sfmax_Q_K\n",
    "        final_output = self.linear_layer(attn_out.reshape((self.batch_size,-1)))\n",
    "\n",
    "        return final_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 10\n",
    "# out_var = 2\n",
    "# input_var = n_var - out_var\n",
    "# input = main_df[sorted(all_cols, reverse=True)[:input_var]].iloc[:batch_size].to_numpy()\n",
    "# Model = AttentionForecastModel(d_k=4, \n",
    "#                                d_v=3, \n",
    "#                                d_emb=5, \n",
    "#                                batch_size=batch_size,\n",
    "#                                seq_len=input_var, \n",
    "#                                out_seq_len=out_var)\n",
    "# Model(input)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train cost: 33.43925895690917\n",
      "train cost: 2.4557674184441556\n",
      "train cost: 2.40232402123511\n",
      "train cost: 2.3536803327500815\n",
      "train cost: 2.3097049467265607\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "OUT_VAR = 5\n",
    "BATCH_SIZE = 10\n",
    "feat_cols = all_cols[OUT_VAR:-1]\n",
    "tgt_cols = all_cols[:OUT_VAR]\n",
    "\n",
    "train_len = int(row_count*0.8)\n",
    "batch_indexes = get_batch_indexes(batch_size=BATCH_SIZE, data_len=row_count, train_len=train_len)\n",
    "\n",
    "Model = AttentionForecastModel(d_k=4, \n",
    "                               d_v=3, \n",
    "                               d_emb=5, \n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               seq_len=len(feat_cols), \n",
    "                               out_seq_len=len(tgt_cols))\n",
    "optimizer = torch.optim.Adam(Model.parameters(), lr = 0.01)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epch_trn_cost = 0\n",
    "    for st_index, end_index in batch_indexes:\n",
    "        batch_data = main_df.iloc[st_index:end_index]\n",
    "        feat_data = batch_data[feat_cols].to_numpy()\n",
    "        tgt_data = torch.tensor(batch_data[tgt_cols].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "        pred_out = Model(feat_data)\n",
    "        cost = nn.functional.huber_loss(pred_out,tgt_data)\n",
    "        epch_trn_cost+=(float(cost.detach())/BATCH_SIZE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch%10==0:\n",
    "        print('train cost:',epch_trn_cost )\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = main_df.iloc[800:]\n",
    "\n",
    "test_feat_data = test_data[feat_cols].to_numpy()\n",
    "test_tgt_data = test_data[tgt_cols].to_numpy()\n",
    "\n",
    "Model.batch_size= row_count-train_len\n",
    "test_pred = Model(test_feat_data).detach().numpy()\n",
    "\n",
    "test_pred_data = pd.DataFrame(np.round(test_pred,2))\n",
    "test_pred_data.columns = [f'pred_x{i}' for i in range(OUT_VAR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test_data =pd.concat([test_data.reset_index(drop=True),test_pred_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.057130084748072595"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error(y_pred=combined_test_data['pred_x0'],\n",
    "                               y_true=combined_test_data['x0'],\n",
    "                               symmetric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>pred_x0</th>\n",
       "      <th>pred_x1</th>\n",
       "      <th>pred_x2</th>\n",
       "      <th>pred_x3</th>\n",
       "      <th>pred_x4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.84</td>\n",
       "      <td>18.37</td>\n",
       "      <td>11.52</td>\n",
       "      <td>7.49</td>\n",
       "      <td>4.99</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2.35</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.84</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>29.900000</td>\n",
       "      <td>18.410000</td>\n",
       "      <td>11.530000</td>\n",
       "      <td>7.620000</td>\n",
       "      <td>5.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.46</td>\n",
       "      <td>15.66</td>\n",
       "      <td>9.85</td>\n",
       "      <td>6.43</td>\n",
       "      <td>4.27</td>\n",
       "      <td>2.91</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>25.520000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>9.830000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>4.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.24</td>\n",
       "      <td>41.76</td>\n",
       "      <td>26.39</td>\n",
       "      <td>17.12</td>\n",
       "      <td>11.31</td>\n",
       "      <td>7.70</td>\n",
       "      <td>5.31</td>\n",
       "      <td>3.79</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>68.209999</td>\n",
       "      <td>41.970001</td>\n",
       "      <td>26.360001</td>\n",
       "      <td>17.480000</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.87</td>\n",
       "      <td>5.43</td>\n",
       "      <td>3.44</td>\n",
       "      <td>2.24</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>5.720000</td>\n",
       "      <td>3.590000</td>\n",
       "      <td>2.390000</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.88</td>\n",
       "      <td>45.93</td>\n",
       "      <td>28.96</td>\n",
       "      <td>18.83</td>\n",
       "      <td>12.41</td>\n",
       "      <td>8.48</td>\n",
       "      <td>5.87</td>\n",
       "      <td>4.14</td>\n",
       "      <td>2.94</td>\n",
       "      <td>2.11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>74.820000</td>\n",
       "      <td>45.990002</td>\n",
       "      <td>28.830000</td>\n",
       "      <td>19.059999</td>\n",
       "      <td>13.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>81.22</td>\n",
       "      <td>49.64</td>\n",
       "      <td>31.06</td>\n",
       "      <td>20.06</td>\n",
       "      <td>13.28</td>\n",
       "      <td>9.05</td>\n",
       "      <td>6.21</td>\n",
       "      <td>4.38</td>\n",
       "      <td>3.11</td>\n",
       "      <td>2.26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>80.300003</td>\n",
       "      <td>49.349998</td>\n",
       "      <td>30.900000</td>\n",
       "      <td>20.389999</td>\n",
       "      <td>13.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>50.49</td>\n",
       "      <td>31.01</td>\n",
       "      <td>19.56</td>\n",
       "      <td>12.54</td>\n",
       "      <td>8.29</td>\n",
       "      <td>5.58</td>\n",
       "      <td>3.89</td>\n",
       "      <td>2.76</td>\n",
       "      <td>1.96</td>\n",
       "      <td>1.41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>48.880001</td>\n",
       "      <td>30.160000</td>\n",
       "      <td>18.969999</td>\n",
       "      <td>12.590000</td>\n",
       "      <td>8.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>20.41</td>\n",
       "      <td>12.43</td>\n",
       "      <td>7.75</td>\n",
       "      <td>4.97</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.299999</td>\n",
       "      <td>11.860000</td>\n",
       "      <td>7.450000</td>\n",
       "      <td>4.950000</td>\n",
       "      <td>3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>9.13</td>\n",
       "      <td>5.60</td>\n",
       "      <td>3.52</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.350000</td>\n",
       "      <td>5.780000</td>\n",
       "      <td>3.630000</td>\n",
       "      <td>2.410000</td>\n",
       "      <td>1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>67.35</td>\n",
       "      <td>41.31</td>\n",
       "      <td>26.16</td>\n",
       "      <td>17.09</td>\n",
       "      <td>11.44</td>\n",
       "      <td>7.83</td>\n",
       "      <td>5.39</td>\n",
       "      <td>3.81</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1.96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>68.599998</td>\n",
       "      <td>42.220001</td>\n",
       "      <td>26.500000</td>\n",
       "      <td>17.559999</td>\n",
       "      <td>12.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        x0     x1     x2     x3     x4    x5    x6    x7    x8    x9  ...  \\\n",
       "0    29.84  18.37  11.52   7.49   4.99  3.40  2.35  1.66  1.18  0.84  ...   \n",
       "1    25.46  15.66   9.85   6.43   4.27  2.91  2.02  1.41  1.00  0.72  ...   \n",
       "2    68.24  41.76  26.39  17.12  11.31  7.70  5.31  3.79  2.72  2.00  ...   \n",
       "3     8.87   5.43   3.44   2.24   1.50  1.01  0.70  0.49  0.35  0.26  ...   \n",
       "4    74.88  45.93  28.96  18.83  12.41  8.48  5.87  4.14  2.94  2.11  ...   \n",
       "..     ...    ...    ...    ...    ...   ...   ...   ...   ...   ...  ...   \n",
       "195  81.22  49.64  31.06  20.06  13.28  9.05  6.21  4.38  3.11  2.26  ...   \n",
       "196  50.49  31.01  19.56  12.54   8.29  5.58  3.89  2.76  1.96  1.41  ...   \n",
       "197  20.41  12.43   7.75   4.97   3.28  2.20  1.51  1.06  0.75  0.55  ...   \n",
       "198   9.13   5.60   3.52   2.29   1.51  1.03  0.71  0.50  0.36  0.26  ...   \n",
       "199  67.35  41.31  26.16  17.09  11.44  7.83  5.39  3.81  2.70  1.96  ...   \n",
       "\n",
       "      x14   x15   x16   x17   x18    pred_x0    pred_x1    pred_x2    pred_x3  \\\n",
       "0    0.18  0.12  0.07  0.03  0.01  29.900000  18.410000  11.530000   7.620000   \n",
       "1    0.13  0.08  0.05  0.03  0.01  25.520000  15.690000   9.830000   6.500000   \n",
       "2    0.36  0.23  0.12  0.06  0.02  68.209999  41.970001  26.360001  17.480000   \n",
       "3    0.05  0.04  0.02  0.01  0.00   9.250000   5.720000   3.590000   2.390000   \n",
       "4    0.39  0.27  0.18  0.08  0.04  74.820000  45.990002  28.830000  19.059999   \n",
       "..    ...   ...   ...   ...   ...        ...        ...        ...        ...   \n",
       "195  0.45  0.29  0.18  0.10  0.02  80.300003  49.349998  30.900000  20.389999   \n",
       "196  0.25  0.17  0.10  0.04  0.01  48.880001  30.160000  18.969999  12.590000   \n",
       "197  0.11  0.07  0.05  0.02  0.00  19.299999  11.860000   7.450000   4.950000   \n",
       "198  0.05  0.04  0.02  0.01  0.00   9.350000   5.780000   3.630000   2.410000   \n",
       "199  0.36  0.22  0.10  0.04  0.00  68.599998  42.220001  26.500000  17.559999   \n",
       "\n",
       "     pred_x4  \n",
       "0       5.23  \n",
       "1       4.45  \n",
       "2      12.00  \n",
       "3       1.65  \n",
       "4      13.07  \n",
       "..       ...  \n",
       "195    13.98  \n",
       "196     8.67  \n",
       "197     3.39  \n",
       "198     1.67  \n",
       "199    12.06  \n",
       "\n",
       "[200 rows x 24 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attention_out = Model.sfmax_Q_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 13])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_attention_out[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(test_attention_out[5].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 10\n",
    "# input = main_df[all_cols].iloc[:batch_size].to_numpy().reshape((batch_size,1,7))\n",
    "\n",
    "# tok_emb = TokenEmbedding(c_in=1, d_emb=5)\n",
    "# output_tokn = tok_emb(torch.tensor(input, dtype=torch.float32))\n",
    "\n",
    "# pos_emb = PositionalEmbedding(d_emb=5)\n",
    "\n",
    "# output_pos = pos_emb(torch.tensor(input))\n",
    "\n",
    "# output_emb = output_tokn.permute(0,2,1)+output_pos\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

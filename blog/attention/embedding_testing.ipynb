{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_IN = 5\n",
    "D_MODEL = 5\n",
    "\n",
    "# tsor = torch.tensor(list(range(C_IN)))\n",
    "# emb_obj = FixedEmbedding(c_in=C_IN,d_model=D_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.zeros(C_IN, D_MODEL).float()\n",
    "w.require_grad = False\n",
    "\n",
    "position = torch.arange(0, C_IN).float().unsqueeze(1)\n",
    "div_term = (torch.arange(0, D_MODEL, 2).float() * -(math.log(10000.0) / D_MODEL)).exp()\n",
    "\n",
    "w[:, 0::2] = torch.sin(position * div_term)[:w[:, 0::2].shape[0],:w[:, 0::2].shape[1]]\n",
    "w[:, 1::2] = torch.cos(position * div_term)[:w[:, 1::2].shape[0],:w[:, 1::2].shape[1]]\n",
    "\n",
    "# emb = nn.Embedding(C_IN, D_MODEL)\n",
    "# emb.weight = nn.Parameter(w, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3960, 0.3152, 0.2343, 0.1534]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_input = torch.tensor([1,2,3,4]).float().reshape((1,-1))\n",
    "\n",
    "conv_layer(tst_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer.kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4709,  0.6300,  0.7892],\n",
       "        [-2.1107, -3.2507, -4.3907]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "m = nn.Conv1d(in_channels=1, out_channels=2, kernel_size=3, stride=1, padding=0)\n",
    "input = torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.float32)\n",
    "output = m(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.6338245868682861, 2.2841799557209015, 2.934535324573517],\n",
       " [1.7435266077518463, 2.3938819766044617, 3.044237345457077]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = np.array([[1, 2, 3, 4, 5],[1, 2, 3, 4, 5]])#.reshape(1,-1)\n",
    "kernel_filt = m.weight.detach().numpy()[0][0]\n",
    "kernel_bias =m.bias.detach().numpy()\n",
    "k_size=3\n",
    "out_lst = []\n",
    "for n_i,arry in enumerate(input):\n",
    "    temp_out = []\n",
    "    for i in range(len(arry)):\n",
    "        slc_lst = arry[i:i+k_size]\n",
    "        if len(slc_lst)==k_size:\n",
    "            temp_out.append(np.sum(slc_lst*kernel_filt)+kernel_bias[n_i])\n",
    "\n",
    "    out_lst.append(temp_out)\n",
    "    # break\n",
    "out_lst\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_IN = 5\n",
    "D_MODEL = 5\n",
    "\n",
    "# tsor = torch.tensor(list(range(C_IN)))\n",
    "# emb_obj = FixedEmbedding(c_in=C_IN,d_model=D_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.zeros(C_IN, D_MODEL).float()\n",
    "w.require_grad = False\n",
    "\n",
    "position = torch.arange(0, C_IN).float().unsqueeze(1)\n",
    "div_term = (torch.arange(0, D_MODEL, 2).float() * -(math.log(10000.0) / D_MODEL)).exp()\n",
    "\n",
    "w[:, 0::2] = torch.sin(position * div_term)[:w[:, 0::2].shape[0],:w[:, 0::2].shape[1]]\n",
    "w[:, 1::2] = torch.cos(position * div_term)[:w[:, 1::2].shape[0],:w[:, 1::2].shape[1]]\n",
    "\n",
    "# emb = nn.Embedding(C_IN, D_MODEL)\n",
    "# emb.weight = nn.Parameter(w, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3960, 0.3152, 0.2343, 0.1534]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_input = torch.tensor([1,2,3,4]).float().reshape((1,-1))\n",
    "\n",
    "conv_layer(tst_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer.kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4709,  0.6300,  0.7892],\n",
       "        [-2.1107, -3.2507, -4.3907]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "m = nn.Conv1d(in_channels=1, out_channels=2, kernel_size=3, stride=1, padding=0)\n",
    "input = torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.float32)\n",
    "output = m(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.6338245868682861, 2.2841799557209015, 2.934535324573517],\n",
       " [1.7435266077518463, 2.3938819766044617, 3.044237345457077]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = np.array([[1, 2, 3, 4, 5],[1, 2, 3, 4, 5]])#.reshape(1,-1)\n",
    "kernel_filt = m.weight.detach().numpy()[0][0]\n",
    "kernel_bias =m.bias.detach().numpy()\n",
    "k_size=3\n",
    "out_lst = []\n",
    "for n_i,arry in enumerate(input):\n",
    "    temp_out = []\n",
    "    for i in range(len(arry)):\n",
    "        slc_lst = arry[i:i+k_size]\n",
    "        if len(slc_lst)==k_size:\n",
    "            temp_out.append(np.sum(slc_lst*kernel_filt)+kernel_bias[n_i])\n",
    "\n",
    "    out_lst.append(temp_out)\n",
    "    # break\n",
    "out_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_emb, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_emb).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_emb, 2).float() * -(math.log(10000.0) / d_emb)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)[:pe[:, 0::2].shape[0],:pe[:, 0::2].shape[1]]\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)[:pe[:, 1::2].shape[0],:pe[:, 1::2].shape[1]]\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 2\n",
    "d_emb = 4\n",
    "\n",
    "pe = torch.zeros(max_len, d_emb).float()\n",
    "pe.require_grad = False\n",
    "\n",
    "position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "div_term = (torch.arange(0, d_emb, 2).float() * -(math.log(10000.0) / d_emb)).exp()\n",
    "\n",
    "pe[:, 0::2] = torch.sin(position * div_term)#[:pe[:, 0::2].shape[0],:pe[:, 0::2].shape[1]]\n",
    "pe[:, 1::2] = torch.cos(position * div_term)#[:pe[:, 1::2].shape[0],:pe[:, 1::2].shape[1]]\n",
    "\n",
    "# pe = pe.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.8415],\n",
       "        [1.0000, 0.5403],\n",
       "        [0.0000, 0.0100],\n",
       "        [1.0000, 0.9999]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8415)\n",
      "tensor(0.9999)\n",
      "tensor(1.0000e-04)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(torch.sin(torch.tensor(1)))\n",
    "print(torch.cos(torch.tensor(1/(10**2))))\n",
    "print(torch.sin(torch.tensor(1/10**4)))\n",
    "print(torch.cos(torch.tensor(1/10**6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "seq_len = 2\n",
    "d_emb = 4\n",
    "POS_EMB = torch.zeros(d_emb, seq_len).float()\n",
    "POS = torch.arange(0, seq_len).repeat(d_emb,1)\n",
    "DIV = 10**(4*torch.arange(0, d_emb).reshape(-1,1).repeat(1,seq_len)*2/d_emb)\n",
    "\n",
    "POS_EMB[0::2, :] = torch.sin(POS/DIV)[0::2, :]\n",
    "POS_EMB[1::2, :] = torch.cos(POS/DIV)[1::2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 8.4147e-01],\n",
       "        [1.0000e+00, 9.9995e-01],\n",
       "        [0.0000e+00, 1.0000e-04],\n",
       "        [1.0000e+00, 1.0000e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_EMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
